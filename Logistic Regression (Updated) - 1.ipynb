{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wf7VI7ClVPYr"
   },
   "source": [
    "# Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn\n",
    "* What is logistic regression?\n",
    "* What is log likelihood and maximum likelihood estimation?\n",
    "* How can you implement logistic regression without using the scikit-learn fucntion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RBTWRejOVPYr"
   },
   "source": [
    "Logistic regression is one of the most widely used machine learning algorithm for classification problems. It is a kind of generalised linear model (you can read more about generalised linear models [here](https://online.stat.psu.edu/stat504/node/216/)), just like the linear regression algorithm. The first question that would come to your mind is why is logistic regression needed when we already have linear regression. This is because linear regression is used to predict a continuous value. But in cases where we need a binary answer, that is, answer in 'Yes' or 'No', we use the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zswHDVU9VPYs"
   },
   "source": [
    "To explain further, let's consider a popular example. We have a dataset of patients with different health parameters using which we have to predict which patient has heart disease and which patient doesn't. In this example we either want an output of 1 for 'Yes' or 0 for 'No'. A linear regression does not suffice in this case. This is where logistic regression comes into picture.\n",
    "\n",
    "**Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable.**\n",
    "\n",
    "In simpler words, given a set of independent feature variables `X`, the logistic regression model gives us an output `y`, which is the probability of the given event occuring. In the above example, `y` would be the probability of our patient having a heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PN3FEK_nVPYt"
   },
   "source": [
    "The output variable `y` of logistic regression is in the form of 'log(odds)'. Therefore, we must know about odds, probability and consequently `log(odds)` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztgzxnJ5VPYu"
   },
   "source": [
    "## Understanding Odds and Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bApgwm2BVPYu"
   },
   "source": [
    "***Odds*** is another way of measuring the probability of an event. Lets say the probability for an event is denoted by *P(E)*\n",
    "\n",
    "Then the odds of the event will be = *P(E) / (1 - P(E))*\n",
    "You might say that the odds and probability of an event are very similar measures. Why at all do we need both? This is because while probability is bounded between 0 to 1, odds can vary from 0 to infinity and log-odds vary between ngative infinity and positive infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ljnvFHzIVPYv"
   },
   "source": [
    "Lets revisit the equation for linear regression : \n",
    "\n",
    "\n",
    "$$Y = B0 + B*X$$\n",
    "\n",
    "where,\n",
    "    \n",
    "   Y = Output variable\n",
    "   \n",
    "   B0 = y-axis intercept \n",
    "   \n",
    "   B1 = set of coeeficients for feature variables X \n",
    "   \n",
    "   X = set of feature variables in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fpZLdG7VPYw"
   },
   "source": [
    "As already mentioned earlier, the output of a logistic regression is log odds. So, the equation for logistic regression, though very similar to linear regression, has one major difference.\n",
    "                    \n",
    "$$Z = B0 + B*X$$\n",
    "\n",
    "As you will notice, all terms are the same except Z. Z here denotes the log odds, which is our output.\n",
    "But how do we find the probability of an event occuring from the log of odds? This is done by using the logit function, also called the sigmoid function.\n",
    "\n",
    "$$Probability = 1 / (1 + e^(-Z))$$\n",
    "                                        \n",
    "The sigmoid curve has a finite limit of :\n",
    "\n",
    "* 0 when x approaches negative infinity\n",
    "* 1 when x approaches positive infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qI800XCVPYw"
   },
   "source": [
    "![sigmoid.png](attachment:sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mlseMpJxVPYx"
   },
   "source": [
    "So far, we have understood the math behind logistic regression, but we still dont know the value of our coeeficients B0 and B1. In linear regression, we used the OLS (Ordinary Least Squares) method to find feature variable coefficients and the intercept. In logistic regression we use the method of Maximum Likelihood Estimation, also known as MLE. \n",
    "We will further see what is MLE and how can we implement it through code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dQ6BkwIVPYy"
   },
   "source": [
    "## Maximum Likelihood Estimation - Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQmdH3StVPYy"
   },
   "source": [
    "There can be infinite number of regression coefficients but the maximum likelihood estimate is that set of regression coefficients for which the probability of getting the data we have observed is maximum. \n",
    "It involves maximizing a likelihood function in order to find the probability distribution and parameters that best explain the observed data.\n",
    "\n",
    "Here we are trying to find values of B {B0, B1, B2 ...} to fit our dataset X and then make an accurate prediction y. Thus we are trying the maximize our probability of observing a dataset X given a range of parameters B for our model.\n",
    "This probability is represented as : *P(X | B)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fp4aTfi3VPYz"
   },
   "source": [
    "'|' denotes conditional probability, but commonly it is denoted by ';'. So *P(X | B)* = *P(X ; B)*.\n",
    "This probability is called the likelihood function and is denoted by L(). Thus our likelihood function is **L(X ; B)**.\n",
    "The aim of MLE is to maximize this function, that is, find *max(L(X ; B))*.\n",
    "\n",
    "If we consider n samples in our dataset {x1, x2, x3..... xn} the likelihood function becomes: \n",
    "                                            \n",
    "                                                   L({x1, x2, x3...xn ; B)\n",
    "This can be unpacked as: \n",
    "                         \n",
    "                                               product i to n L(xi ; B), or,\n",
    " $$\\prod_{i = 1}^{n} L(x_i ; B)$$ \n",
    "\n",
    "As multplying so many probabilities together can be complex and unstable, we take log of the likelihood function. So after maximizing our expression changes to: \n",
    "\n",
    "                                                sum i to n logL(xi ; B) \n",
    "$$\\sum_{i=1}^{n} logL(x_i ; B)$$                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08k7_UbzVPY0"
   },
   "source": [
    "In a logistic regression model, B is the parameters of our model. Using this model we will make our predictions from the given dataset. Let's take the example of supervised learning. We need to find the conditional probability of predicting output y given X (P(y | X)). The likelihood function changes to:\n",
    "\n",
    "                                      maximize sum i to n logL(P(yi | xi) ; B), or,\n",
    "$$\\operatorname{argmax}\\sum_{i=1}^{n} logL(x_i ; B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQqL1F3dVPY0"
   },
   "source": [
    "To use the MLE function, we need a probability distribution of our output. The ouputs of a logistic regression can only have 2 results, 1 or 0. So we use a Bernoulli distribution (read more about Bernoulli distribution [here](https://towardsdatascience.com/understanding-bernoulli-and-binomial-distributions-a1eef4e0da8f)).\n",
    "The Bernoulli distribution has a single parameter: the probability of a successful outcome (p).\n",
    "* P(y=1) = p\n",
    "* P(y=0) = 1 – p\n",
    "\n",
    "The expected value (mean) of Bernoulli distribution is calculated as: \n",
    "\n",
    "                                            mean = p * 1 + (1 – p) * 0\n",
    "\n",
    "This provides the basis for the likelihood function for a specific input, where the output probability is given by the model (yhat) and the actual label (y) is given from the dataset.\n",
    "\n",
    "                                     likelihood = yhat * y + (1 – yhat) * (1 – y), or,\n",
    "$$LL = y\\hat{} * y + (1 – y\\hat{}) * (1 – y)$$\n",
    "\n",
    "This function will always return a large probability when the model is close to the matching class value, and a small value when it is far away, for both y=0 and y=1 cases.\n",
    "\n",
    "A small implementation is shown below : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_g5NKNKUVPY1",
    "outputId": "058c6b11-b9b3-4334-eddd-3b3764b03f1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=1.0, yhat=0.9, likelihood: 0.900\n",
      "y=1.0, yhat=0.1, likelihood: 0.100\n",
      "y=0.0, yhat=0.1, likelihood: 0.900\n",
      "y=0.0, yhat=0.9, likelihood: 0.100\n"
     ]
    }
   ],
   "source": [
    "# test of Bernoulli likelihood function\n",
    "\n",
    "# likelihood function for Bernoulli distribution\n",
    "def likelihood(y, yhat):\n",
    "    return yhat * y + (1 - yhat) * (1 - y)\n",
    "\n",
    "# test for y=1\n",
    "y, yhat = 1, 0.9\n",
    "print('y=%.1f, yhat=%.1f, likelihood: %.3f' % (y, yhat, likelihood(y, yhat)))\n",
    "y, yhat = 1, 0.1\n",
    "print('y=%.1f, yhat=%.1f, likelihood: %.3f' % (y, yhat, likelihood(y, yhat)))\n",
    "# test for y=0\n",
    "y, yhat = 0, 0.1\n",
    "print('y=%.1f, yhat=%.1f, likelihood: %.3f' % (y, yhat, likelihood(y, yhat)))\n",
    "y, yhat = 0, 0.9\n",
    "print('y=%.1f, yhat=%.1f, likelihood: %.3f' % (y, yhat, likelihood(y, yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvdZKCbdVPY5"
   },
   "source": [
    "We can update the likelihood function using the log to transform it into a log-likelihood function:\n",
    "\n",
    "                               log-likelihood = log(yhat) * y + log(1 – yhat) * (1 – y), or,\n",
    "$$log(LL) = log(y\\hat{}) * y + log(1 – y\\hat{}) * (1 – y)$$\n",
    "Finally, we can sum the likelihood function across all examples in the dataset to maximize the likelihood:\n",
    "\n",
    "                        maximize sum i to n log(yhat_i) * yi + log(1 – yhat_i) * (1 – yi), or,\n",
    "$$\\operatorname{argmax}\\sum_{i=1}^{n} log(y\\hat{}_i) * y_i + log(1 – y\\hat{}_i) * (1 – y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhAw_cCgVPY6"
   },
   "source": [
    "## Estimating coefficients manually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cewjYgkKVPY6"
   },
   "source": [
    "To estimate the coefficients of our model from scratch, you will first need a function that can predict the output probability of our model from `yhat`.\n",
    "Below is a function named predict() that predicts an output value for a row given a set of coefficients.\n",
    "\n",
    "The first coefficient is always the intercept, also called the bias or B0 as it is standalone and not responsible for a specific input value. The equation of our model in this case is the standard equation : \n",
    "$$Z = B0 + B1 * x1 + B2 * x2$$\n",
    "where, B0 = intercept on y-axis and B1 & B2 = coefficient for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOA5ATJ6VPY7"
   },
   "outputs": [],
   "source": [
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0] # This denotes the term B0\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i] # This statement adds the term B1 * X\n",
    "    return 1.0 / (1.0 + exp(-yhat)) # We return the probability using sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32k6qT1eVPY-"
   },
   "source": [
    "To efficiently test our function and estimate coefficients you also need a create a dataset with values of X and y.\n",
    "Below is a random dataset that will be further used in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKo8yyG-VPY-"
   },
   "outputs": [],
   "source": [
    "dataset = [[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcIfWWdrVPZB"
   },
   "source": [
    "The dataset we have created above is an array with the following values for x1, x2 and y :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLHa9mE9VPZB"
   },
   "outputs": [],
   "source": [
    "    X1        X2        Y\n",
    "2.7810836  2.550537003  0\n",
    "1.465489372 2.362125076 0\n",
    "3.396561688 4.400293529 0\n",
    "1.38807019  1.850220317 0\n",
    "3.06407232  3.005305973 0\n",
    "7.627531214 2.759262235 1\n",
    "5.332441248 2.088626775 1\n",
    "6.922596716 1.77106367  1\n",
    "8.675418651 -0.24206865 1\n",
    "7.673756466 3.508563011 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXQ8FkYOVPZD"
   },
   "source": [
    "The values of y are only 0 or 1, because the model we are trying to build is that of a logistic regression which is used for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fik2WzQgVPZE"
   },
   "source": [
    "The next step is to determine the coefficients B0, B1 and B2 which is done using stochastic gradient descent (read further about gradient descent [here](https://machinelearningmastery.com/gradient-descent-for-machine-learning/).\n",
    "You can calculate the new coefficient values using a simple update equation.\n",
    "\n",
    "                      B{t) = B + alpha * (y(t) – yhat(t)) * yhat(t) * (1 – yhat(t)) * x, or,\n",
    "$$ B(t) = B(t) + \\alpha * (y – y\\hat{}) * y\\hat{} * (1 – y\\hat{}) * x$$\n",
    "The special coefficient at the beginning of the list, also called the intercept, is updated in a similar way, except without an input as it is not associated with a specific input value:\n",
    "\n",
    "                      B0(t+1) = B0(t) + alpha * (y(t) - yhat(t)) * yhat(t) * (1 - yhat(t)), or,\n",
    "$$B0(t+1) = B0(t) + \\alpha * (y(t) - y\\hat{}(t)) * y\\hat{}(t) * (1 - y\\hat{}(t))$$\n",
    "\n",
    "Here, alpha is the learning rate of gradient descent and controls how much the coefficients (and therefore the model) changes or learns each time it is updated. Good values of alpha range between 0.1 and 0.3. For this example we will assume $$ \\alpha = 0.1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDSo7j9tVPZE"
   },
   "source": [
    "Another parameter for carrying out gradient descent is number of epochs, which is he number of times to run through the training data while updating the coefficients.\n",
    "We now have the learning rate alpha and the number of epochs. The function below will take in these parameters along with the dataset created above to give us a set of estimated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr6kP7_TVPZF"
   },
   "outputs": [],
   "source": [
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef) # You call the predict function here that was created above to get yhat\n",
    "            error = row[-1] - yhat # the term of y-yhat\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat) # Intercept B0 is estimated\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i] #B1 and B2 are estimated\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svyKRgfVVPZI"
   },
   "source": [
    "Now you can test this function on the created dataset. using `predict()` and `coefficients_sgd()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4eKu19xVPZI",
    "outputId": "6b93c9c3-47fa-483f-b717-dc9b525e9043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4067126400924392, 0.8209365836324548, -1.1495205736938134]\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    " \n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-yhat))\n",
    " \n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = row[-1] - yhat\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coef\n",
    " \n",
    "# Calculate coefficients\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "          [1.465489372,2.362125076,0],\n",
    "          [3.396561688,4.400293529,0],\n",
    "          [1.38807019,1.850220317,0],\n",
    "          [3.06407232,3.005305973,0],\n",
    "          [7.627531214,2.759262235,1],\n",
    "          [5.332441248,2.088626775,1],\n",
    "          [6.922596716,1.77106367,1],\n",
    "          [8.675418651,-0.242068655,1],\n",
    "          [7.673756466,3.508563011,1]]\n",
    "l_rate = 0.1\n",
    "n_epoch = 30\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CzsIzzZeVPZK"
   },
   "source": [
    "As you can see above, the values for B0, B1 and B2 are respectively -0.4067126400924392, 0.8209365836324548, -1.1495205736938134. But to check if the model is giving the correct outputs, you can use the inbuilt `LogisticRegression()` model from scikit-learn and check the value of coeeficients. \n",
    "To do this you will have to divide your dataset into X and y, as shown below : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJCOaf4DVPZL"
   },
   "outputs": [],
   "source": [
    "X = [[2.7810836, 2.550537003],  \n",
    "[1.465489372, 2.362125076], \n",
    "[3.396561688, 4.400293529], \n",
    "[1.38807019, 1.850220317], \n",
    "[3.06407232, 3.005305973], \n",
    "[7.627531214, 2.759262235], \n",
    "[5.332441248, 2.088626775], \n",
    "[6.922596716, 1.77106367],  \n",
    "[8.675418651, -0.24206865], \n",
    "[7.673756466, 3.508563011]]\n",
    "\n",
    "y = [0,0,0,0,0,1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e6R_KWueVPZN"
   },
   "source": [
    "Now we can import the `LogisticRegression()` model and use it on the dataset to find B0, B1 and B2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUP1w4COVPZO"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear') # Not specifying a solver will throw us a warning.\n",
    "                                             # You can read more about the model's parameters from \n",
    "                                             # scikit-learn's documentation on logistic Regression\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lf-dqeQDVPZQ",
    "outputId": "f91faa1a-024d-44d8-d8b5-ca6bed42af37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.43104649] [[ 0.80500337 -1.1224275 ]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.intercept_, clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1bLrW1IVPZT"
   },
   "source": [
    "As you can see, The coefficients obtained from scikit-learn logistic regression model {-0.43104649, 0.80500337, -1.1224275} are very close to coefficients obtained from the function built by us to estimate B {-0.4067126400924392, 0.8209365836324548, -1.1495205736938134}. This proves the correcteness of our function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JG5DYbfNVPZT"
   },
   "source": [
    "#### References : \n",
    "1. Machine Learning Mastery [blog](https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/)\n",
    "2. Towards Data Science [blog](https://towardsdatascience.com/real-world-implementation-of-logistic-regression-5136cefb8125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGZJ302_VPZU"
   },
   "source": [
    "                                                -------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Logistic Regression (Updated) - 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
