{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing AdaBoost in Python\n",
    "\n",
    "In this tutorial you will learn : \n",
    "* What is boosting?\n",
    "* How does AdaBoost work?\n",
    "* Implementation of Adaboost in Python\n",
    "\n",
    "## What is boosting?\n",
    "Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers. We first build a weak model, and then build a second model based on the errors from the first model. This process is repeated over and over again until we build a classifier that can make predictions accurately and the error has been minimised.\n",
    "\n",
    "Boosting differs from bagging in that it trains the weak learners sequentially, and not in parallel.\n",
    "The process can be described as such :\n",
    "\n",
    "- Train the model h1 on the whole set\n",
    "- Train the model h2 with exaggerated data on the regions in which h1 performs poorly\n",
    "- Train the model h3 with exaggerated data on the regions in which h1 ≠ h2 … and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does AdaBoost work?\n",
    "\n",
    "AdaBoost is one of the first boosting algorithms to have been introduced. It is mainly used for classification and the base learner (the machine learning algorithm that is boosted) is usually a decision tree with only one level, also called as stumps.\n",
    "AdaBoost makes use of weighted errors to build a strong classifier from a series of weak classifiers.\n",
    "It works in the following steps:\n",
    "\n",
    "1. Initially, Adaboost selects a training subset randomly.\n",
    "2. It iteratively trains the AdaBoost machine learning model by selecting the training set based on the accurate prediction of the last training.\n",
    "3. It assigns the higher weight to wrong classified observations so that in the next iteration these observations will get the high probability for classification.\n",
    "4. Also, It assigns the weight to the trained classifier in each iteration according to the accuracy of the classifier. The more accurate classifier will get high weight.\n",
    "5. This process iterates until the complete training data fits without any error or until reached to the specified maximum number of estimators.\n",
    "\n",
    "### Weighted errors\n",
    "The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations.\n",
    "The weight of each sample for the first iteration is :\n",
    "                \n",
    "                                                  weight(xi) = 1/n\n",
    "where,   \n",
    "xi = i'th sample \\\n",
    "n = number of samples\n",
    "\n",
    "### Training the Model\n",
    "A weak classifier (decision stump) is prepared on the training data using the weighted samples. Only binary (two-class) classification problems are supported, so each decision stump makes one decision on one input variable and outputs a +1.0 or -1.0 value for the first or second class value.\n",
    "\n",
    "The **misclassification rate** is calculated for the trained model. \n",
    "\n",
    "                                        error = sum(w(i) * terror(i)) / sum(w)\n",
    "\n",
    "\n",
    "which is the weighted sum of the misclassification rate, where w is the weight for sample i and terror is the prediction error for sample i which is 1 if misclassified and 0 if correctly classified.\n",
    "\n",
    "For example, if we had 3 samples with the weights 0.01, 0.5 and 0.2. The predicted values were -1, -1 and -1, and the actual output variables in the instances were -1, 1 and -1, then the terrors would be 0, 1, and 0. The misclassification rate would be calculated as:\n",
    "\n",
    "error = (0.01* 0 + 0.5* 1 + 0.2* 0) / (0.01 + 0.5 + 0.2)\n",
    "\n",
    "or,\n",
    "\n",
    "error = 0.704\n",
    "\n",
    "A **stage value** is calculated for the trained model which provides a weighting for any predictions that the model makes. The stage value for a trained model is calculated as follows:\n",
    "\n",
    "                                            stage = ln((1-error) / error)\n",
    "\n",
    "where stage is the stage value used to weight predictions from the model, ln() is the natural logarithm and error is the misclassification error for the model. The effect of the stage weight is that more accurate models have more weight or contribution to the final prediction.\n",
    "\n",
    "The training weights are updated giving more weight to incorrectly predicted outcomes, and less weight to correctly predicted outcomes.\n",
    "\n",
    "For example, the **weight of one training instance (w)** is updated using:\n",
    "\n",
    "                                               w = w * exp(stage * terror)\n",
    "\n",
    "Where w is the weight for a specific training instance, exp() is the numerical constant e raised to (stage + terror), stage is the misclassification rate for the weak classifier and **terror** is the error the weak classifier made predicting the output variable for the training instance, evaluated as:\n",
    "\n",
    "                                            terror = 0 if(y == p), otherwise 1\n",
    "\n",
    "Where y is the output variable for the sample xi and p is the prediction from the weak learner.\n",
    "\n",
    "This has the effect of not changing the weight if the training instance was classified correctly and making the weight slightly larger if the weak learner misclassified the instance. After updation of weights this iteration is done again, and again until the number of maximum iterations have been reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of AdaBoost in Python\n",
    "\n",
    "You can implement AdaBoost on any dataset that has a binary output to be predicted. Here we use the inbuilt dataset from sklearn, 'iris', where there are 50 entries and 3 classes of flowers. Each entry belongs to a type a flower plant. You first have to import all relevant libraries from `sklearn` library and then use the classifier `AdaBoostClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import datasets\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have to split the data into training and test sets to make predictions and calculate the accuracy of predictions made by Adaboost algorithm. This is done using `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement AdaBoost on the dataset we have loaded. There are three main parameters of the `AdaBoostClassifier()`. They are:\n",
    "* `n_estimators` - The maximum number of weak learners that AdaBoost is allowed to build.\n",
    "* `learning_rate` - Learning rate shrinks the contribution of each classifier by learning_rate.\n",
    "* `base_estimator` - This is the algorithm which is boosted to build the complete model. By default it is set as  `DecisionTreeClassifier(max_depth=1)`. The depth here signifies the number of levels in the descision tree. As deptht is 1 in this case, it means we are working with stumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adaboost classifer object\n",
    "abc = AdaBoostClassifier(n_estimators=50,\n",
    "                         learning_rate=1)\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final test of our model is finding the accuracy of its predictions. We compare `y_pred` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy achieved is 93.33 percent, which is a pretty good score for a basic boosting algorithm like AdaBoost. This is a proof of how effective boosting is in increasing the perfomance of even simple algorithms such as decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Machine Learning Mastery [blog](https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/)\n",
    "* DataCamp [tutorial](https://www.datacamp.com/community/tutorials/adaboost-classifier-python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
